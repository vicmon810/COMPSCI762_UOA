{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fd3c9f",
   "metadata": {},
   "source": [
    "# UoA Copcsi 765 Assignemnt 1\n",
    "Shuo Mao 437681250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673443c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "file_path = 'website-phishing.csv'\n",
    "web = pd.read_csv(file_path)\n",
    " \n",
    "bcp = pd.read_csv(\"BCP.csv\")\n",
    "arrhy = pd.read_csv(\"arrhythmia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "web.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrhy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4669d7",
   "metadata": {},
   "source": [
    "By manually brife view the row data of each one, that I discover in the dataset of arrhythmia have missing value repersenting by question mark `?`, thus I'll read through all data and replace `?` by `Null` value, so the machineism can process feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' with np.nan for the entire DataFrame\n",
    "arrhy.replace('?', np.nan, inplace=True)\n",
    "# Convert all columns to numeric, errors='coerce' will convert errors (non-convertible values) to NaN\n",
    "data = arrhy.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace NaN values with the mean of their respective columns for the entire DataFrame\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "#generate output csv file for following steps\n",
    "data.to_csv('new_arrhy.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1e524",
   "metadata": {},
   "source": [
    "# 1: Decision Stump\n",
    "A decision stump is a decision tree with a depth of 1. It splits the dataset using the best feature based on a criterion like Gini impurity or information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        # Initialize the decision stump with default values\n",
    "        self.threshold = None  # The value to decide if a data point goes left or right\n",
    "        self.feature_index = None  # The index of the feature to split on\n",
    "        self.left_value = None  # The most common class on the left side of the stump\n",
    "        self.right_value = None  # The most common class on the right side of the stump\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        best_info_gain = -1  # Initialize the best information gain to a value that will be updated\n",
    "        # Iterate over all features in the dataset\n",
    "        for feature_index in range(X.shape[1]): # in this case feature_index repersending columns in row dataset\n",
    "            thresholds = np.unique(X[:, feature_index])  # Get all unique values of the feature as potential thresholds\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the information gain of splitting on this feature at this threshold\n",
    "                info_gain = self.information_gain(y, X[:, feature_index], threshold)\n",
    "                if info_gain > best_info_gain:\n",
    "                    # If this split provides a better information gain, update the stump's parameters\n",
    "                    best_info_gain = info_gain\n",
    "                    self.threshold = threshold\n",
    "                    self.feature_index = feature_index\n",
    "        \n",
    "        # After finding the best split, determine the output value (class) for each side of the split\n",
    "        # This uses the mean of the target values, which implicitly handles binary classification (0 and 1)\n",
    "        self.left_value = np.round(np.mean(y[X[:, self.feature_index] <= self.threshold]))\n",
    "        self.right_value = np.round(np.mean(y[X[:, self.feature_index] > self.threshold]))\n",
    "        \n",
    "        # Determine the value for the left and right child nodes\n",
    "        self.left_value = np.round(np.mean(y[X[:, self.feature_index] <= self.threshold]))\n",
    "        self.right_value = np.round(np.mean(y[X[:, self.feature_index] > self.threshold]))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Generate predictions based on the threshold and feature index\n",
    "        predictions = np.where(X[:, self.feature_index] <= self.threshold, self.left_value, self.right_value)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def information_gain(self, y, feature_column, threshold):\n",
    "        parent_entropy = self.entropy(y)  # Calculate the entropy before the split\n",
    "        \n",
    "        # Create masks to split the data based on the threshold\n",
    "        left_child_index = feature_column <= threshold\n",
    "        right_child_index = feature_column > threshold\n",
    "        \n",
    "        # Ensure both sides of the split contain samples\n",
    "        if np.any(left_child_index) and np.any(right_child_index):\n",
    "            n = len(y)\n",
    "            n_left = np.sum(left_child_index)  # Number of samples in the left split\n",
    "            n_right = np.sum(right_child_index)  # Number of samples in the right split\n",
    "            # Weighted average of child entropies\n",
    "            child_entropy = (n_left / n) * self.entropy(y[left_child_index]) + (n_right / n) * self.entropy(y[right_child_index])\n",
    "            info_gain = parent_entropy - child_entropy  # Information gain is reduction in entropy\n",
    "            return info_gain\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        if np.any(y < 0):  # Handle datasets where the negative class is labeled as -1\n",
    "            y = np.where(y == -1, 0, 1)\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the dataset has been loaded into a DataFrame named data\n",
    "# and the necessary preprocessing has been done.\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "stump = DecisionStump()\n",
    "\n",
    "\n",
    "X = web.iloc[:, :-1].values# Selecting all expect y(class)\n",
    "y = web.iloc[:, -1].values# Selecting Class\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "\n",
    "#fit the decision stump\n",
    "\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "# Use the stump to make predictions\n",
    "predictions = stump.predict(X_test)\n",
    "# Optionally, evaluate the stump's performance\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'Accuracy of the decision stump in website-phisihing dataset: {accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = bcp.iloc[:, :-1].values\n",
    "y = bcp.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=78)\n",
    "stump.fit(X_train, y_train)\n",
    "# Use the stump to make predictions\n",
    "predictions = stump.predict(X_test)\n",
    "# Optionally, evaluate the stump's performance\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'Accuracy of the decision stump in BCP dataset: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrhy_aft = pd.read_csv(\"new_arrhy.csv\")\n",
    "\n",
    "X= arrhy_aft.iloc[:,:-1].values\n",
    "y = arrhy_aft.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "stump.fit(X_train, y_train)\n",
    "# Use the stump to make predictions\n",
    "predictions = stump.predict(X_test)\n",
    "# Optionally, evaluate the stump's performance\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'Accuracy of the decision stump in arrhythmia dataset: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc00888",
   "metadata": {},
   "source": [
    "After applying the Decision Stump method to each raw dataset, it was discovered that the Decision Stump works effectively for basic binary classification tasks. For example, in the website-phishing dataset, which has just two possible outcomes `(-1, 1)`, and the BCP dataset `(with outcomes 2,4)`, the accuracy rates were pretty high, about `0.89`.This highlights the effectiveness of Decision Stumps in binary classification scenarios. In contrast, the Decision Stump performs substantially worse in non-binary classification cases, such as the arrhythmia dataset, which has `13` alternative outcomes. In this example, the accuracy dropped to a meager `0.022`, indicating a `2%` probability of making a right forecast. This striking disparity demonstrates Decision Stumps' limits in handling increasingly complicated, non-binary categorization tasks.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a52d5a",
   "metadata": {},
   "source": [
    "# 2: Unpruned Decision Tree\n",
    "An unpruned decision tree grows until all leaves are pure or until every leaf contains a minimum number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32fd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        # Node class to store information about each point in the decision tree.\n",
    "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
    "        self.threshold = threshold  # Threshold value for the split at this node\n",
    "        self.left = left  # Reference to the left child node\n",
    "        self.right = right  # Reference to the right child node\n",
    "        self.info_gain = info_gain  # Information gain from the split at this node\n",
    "        self.value = value  # The class value if this is a leaf node, else None\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        # Initialize the decision tree classifier with stopping conditions.\n",
    "        self.root = None  # Root node of the tree, initially None\n",
    "        self.min_samples_split = min_samples_split  # Minimum number of samples required to split a node\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree\n",
    "\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        # Recursively builds the decision tree from the given dataset.\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]  # Split dataset into features (X) and target (Y)\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # Base case: stop splitting if minimum samples or maximum depth criteria are met\n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"] > 0:  # Ensure positive information gain for splitting\n",
    "                # Recursively build left and right subtrees\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        return Node(value=self.calculate_leaf_value(Y))  # Leaf node with class prediction\n",
    "\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\" This method identifies the optimal feature and threshold for splitting the dataset into two child nodes. It aims to maximize the information gain from the split.\"\"\"\n",
    "        # Initialize the dictionary to store the best split found during the process\n",
    "        best_split = {}\n",
    "        # Initialize the maximum information gain to a very low number\n",
    "        max_info_gain = -float(\"inf\")\n",
    "         # Iterate over all features in the dataset\n",
    "        for feature_index in range(num_features):\n",
    "            # Extract all values for the current feature\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            # Identify all unique values of the feature to consider as potential thresholds\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "             # Test each threshold to find the best split for the current feature\n",
    "            for threshold in possible_thresholds:\n",
    "                # Split the dataset into two parts: one part where the feature values are less than or equal to the threshold\n",
    "                 # and another part where the feature values are greater than the threshold\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # Ensure both left and right splits contain data\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    # Extract the target values for the whole dataset, and each of the splits\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # Calculate the information gain achieved by the current split\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # If the information gain from the current split is higher than the maximum found so far,\n",
    "                    # update the best split to this split\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "        return best_split\n",
    "\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        # Creates an empty array for the left dataset. This will hold all rows\n",
    "        # where the value in the specified feature column is less than or equal\n",
    "        # to the threshold.\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold]) \n",
    "        # Similarly, creates an empty array for the right dataset. This will\n",
    "        # contain all rows where the value in the specified feature column is\n",
    "        # greater than the threshold.\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        # Returns the two subsets. These will be used as the new datasets for\n",
    "        # the subsequent recursive calls to build the left and right subtrees,\n",
    "        # respectively.\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):  \n",
    "        # Calculate the proportion of samples that go to the left and right child nodes.\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        # If the mode is set to \"gini\", use the Gini impurity measure to calculate information gain.\n",
    "        if mode == \"gini\":\n",
    "            # Information gain is the impurity of the parent node minus the weighted sum of the impurity\n",
    "            # of the left and right child nodes.\n",
    "            gain = self.gini_index(parent) - (weight_l * self.gini_index(l_child) + weight_r * self.gini_index(r_child))\n",
    "        else:\n",
    "            # If the mode is not \"gini\" (default is \"entropy\"), use the entropy measure to calculate information gain.\n",
    "            gain = self.entropy(parent) - (weight_l * self.entropy(l_child) + weight_r * self.entropy(r_child))\n",
    "        \n",
    "        return gain\n",
    "\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        # Identify the unique class labels in the dataset.\n",
    "        class_labels = np.unique(y)\n",
    "        # Initialize entropy to zero. Entropy will accumulate the disorder measure for each class.\n",
    "        entropy = 0\n",
    "        # Iterate over each class label. \n",
    "        for cls in class_labels:\n",
    "            # Calculate the proportion (p_cls) of instances in the dataset belonging to the current class.\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            # Update the entropy using the entropy formula part associated with this class.\n",
    "            # The formula for entropy of a class is: -p * log2(p), where p is the class proportion.\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        # Return the total entropy, which represents the disorder within the entire dataset.\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    def gini_index(self, y):      \n",
    "        # Identify the unique class labels in the dataset.\n",
    "        class_labels = np.unique(y)\n",
    "        # Initialize gini to zero. This will accumulate the squared proportion of each class.\n",
    "        gini = 0\n",
    "        # Iterate over each class label.\n",
    "        for cls in class_labels:\n",
    "            # Calculate the proportion (p_cls) of instances in the dataset belonging to the current class.\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            # Accumulate the square of the proportion of the current class to the gini score.\n",
    "            gini += p_cls**2\n",
    "        # Gini impurity is calculated as 1 minus the accumulated score.\n",
    "        # This transformation ensures that a higher value indicates higher impurity.\n",
    "        return 1 - gini\n",
    "\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        # Convert Y to a list if it's not already one. This ensures compatibility with the 'max' function and 'count' method.\n",
    "        Y = list(Y)\n",
    "        # Use the 'max' function with a key that counts the occurrences of each class label in Y.\n",
    "        # This returns the class label that appears most frequently in Y.\n",
    "        return max(Y, key=Y.count)\n",
    "\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''     \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # Ensures that Y is in the correct shape.\n",
    "        # If Y is a 1-dimensional array (a common format for labels), it reshapes it to a 2-dimensional array with a single column.\n",
    "        # This uniform shape is necessary for concatenating Y with X.\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1, 1)\n",
    "        # This combined dataset format is convenient for the subsequent operations of splitting and building the tree.\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        # Initiates the recursive process of building the decision tree.\n",
    "        # The build_tree method will be called with the entire dataset,\n",
    "        # and it will recursively split the dataset on the best features and thresholds,\n",
    "        # until it meets the stopping criteria to create leaf nodes.\n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # This line iterates over each example in the dataset X. For each example (x),\n",
    "        # it calls the make_prediction method, starting from the root of the tree.\n",
    "        # The result is a list of predictions corresponding to each example in X.\n",
    "        predictions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        # Checks if the current node (tree) is a leaf node by verifying if it has a non-None value.\n",
    "        # Leaf nodes in a decision tree do not have further splits and contain a class value (prediction).\n",
    "        if tree.value is not None:\n",
    "            return tree.value  # If it's a leaf node, return the value (class label) of the leaf node.\n",
    "        # Retrieves the feature value from the instance x for the feature at index tree.feature_index.\n",
    "        # This value is used to decide whether to move left or right down the tree.\n",
    "        feature_val = x[tree.feature_index]\n",
    "        # If the feature value is less than or equal to the threshold of the current node,\n",
    "        # the method recursively calls itself to move to the left child of the current node.\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            # If the feature value is greater than the threshold of the current node,\n",
    "            # the method recursively calls itself to move to the right child of the current node.\n",
    "            return self.make_prediction(x, tree.right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c76e0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading website-phishing \n",
    "file_path = 'website-phishing.csv'\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0975806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9249208502939846\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a090ac4",
   "metadata": {},
   "source": [
    "In this investigation, I used `min_samples_split = 2` for our classification issue, which corresponds to the binary character of our results (true or false). This setting is often beneficial in such situations. For `max_depth`, I chose a depth of 5. The objective for this decision is to create a balance between model complexity and computing efficiency while avoiding overfitting. A tree that is overly deep may result in overfitting, which occurs when the model learns noise from the training data rather than the real signal, as well as considerably increasing computation time. In contrast, a shallow tree, while quicker, may not capture the underlying patterns well, resulting in underfitting.\n",
    "\n",
    "I ran tests with `max_depth` values of 2, 5, 8, and 12. A depth of two was insufficient to capture the intricacies of the data. While a depth of 8 produced more accurate results, the longer calculating time was a considerable disadvantage. A depth of 12 resulted in overfitting concerns, indicating that the model was too complicated for our data. Thus, a depth of 5 emerged as the best option, giving a reasonable balance of model accuracy and efficiency, ensuring the model is sophisticated enough to discover meaningful patterns without spending too much time on computations or fitting to noise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37354102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting tree structure of web data\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80956d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading BCP\n",
    "file_path = 'BCP.csv'\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58c90c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9197080291970803\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting tree \n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "725c544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading arrhythmia\n",
    "file_path = 'new_arrhy.csv'#Notice do not loading original dataset program will crash\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4200db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tree\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fe57e",
   "metadata": {},
   "source": [
    "\"After implementing the Decision Tree algorithm on each row of data, observations indicate a distinct performance pattern across different types of classification tasks. In simple binary classification scenarios, the improvement in performance is marginal. However, a significant enhancement is observed in non-binary classification tasks, where the accuracy rate dramatically increased from `0.022` to `0.670`. This improvement aligns with expectations, highlighting the Decision Tree's capability to handle complex classification problems with multiple classes more effectively than simpler, binary classification tasks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c075754",
   "metadata": {},
   "source": [
    "# 3: Pruned Decision Tree\n",
    "Pruning can be done using several strategies like reduced error pruning (post-pruning) or setting a maximum depth (pre-pruning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value\n",
    "\n",
    "class PurningDecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.label_mapping = {}\n",
    "        self.most_common_class = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Map labels to non-negative integers if necessary\n",
    "        self.label_mapping, mapped_Y = self._map_labels(Y)\n",
    "        # Calculate and set the most common class (using the mapped labels)\n",
    "        self.most_common_class = np.bincount(mapped_Y.flatten()).argmax()\n",
    "        dataset = np.concatenate((X, mapped_Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    \n",
    "    def _evaluate_error(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        error = np.mean(predictions != Y.flatten())  # Flattening Y to ensure shape compatibility\n",
    "        return error\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._make_prediction(x, self.root) for x in X]\n",
    "        # Reverse map labels to original\n",
    "        return np.array([self.label_mapping.get(pred, pred) for pred in predictions])\n",
    "\n",
    "    def make_prediction(self, x, tree):\n",
    "        # Checks if the current node (tree) is a leaf node by verifying if it has a non-None value.\n",
    "        # Leaf nodes in a decision tree do not have further splits and contain a class value (prediction).\n",
    "        if tree.value is not None:\n",
    "            return tree.value  # If it's a leaf node, return the value (class label) of the leaf node.\n",
    "        # Retrieves the feature value from the instance x for the feature at index tree.feature_index.\n",
    "        # This value is used to decide whether to move left or right down the tree.\n",
    "        feature_val = x[tree.feature_index]\n",
    "        # If the feature value is less than or equal to the threshold of the current node,\n",
    "        # the method recursively calls itself to move to the left child of the current node.\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            # If the feature value is greater than the threshold of the current node,\n",
    "            # the method recursively calls itself to move to the right child of the current node.\n",
    "            return self.make_prediction(x, tree.right)\n",
    "\n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        # Recursively builds the decision tree from the given dataset.\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]  # Split dataset into features (X) and target (Y)\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # Base case: stop splitting if minimum samples or maximum depth criteria are met\n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"] > 0:  # Ensure positive information gain for splitting\n",
    "                # Recursively build left and right subtrees\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        return Node(value=self.calculate_leaf_value(Y))  # Leaf node with class prediction\n",
    "\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\" This method identifies the optimal feature and threshold for splitting the dataset into two child nodes. It aims to maximize the information gain from the split.\"\"\"\n",
    "        # Initialize the dictionary to store the best split found during the process\n",
    "        best_split = {}\n",
    "        # Initialize the maximum information gain to a very low number\n",
    "        max_info_gain = -float(\"inf\")\n",
    "         # Iterate over all features in the dataset\n",
    "        for feature_index in range(num_features):\n",
    "            # Extract all values for the current feature\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            # Identify all unique values of the feature to consider as potential thresholds\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "             # Test each threshold to find the best split for the current feature\n",
    "            for threshold in possible_thresholds:\n",
    "                # Split the dataset into two parts: one part where the feature values are less than or equal to the threshold\n",
    "                 # and another part where the feature values are greater than the threshold\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # Ensure both left and right splits contain data\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    # Extract the target values for the whole dataset, and each of the splits\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # Calculate the information gain achieved by the current split\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # If the information gain from the current split is higher than the maximum found so far,\n",
    "                    # update the best split to this split\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "        return best_split\n",
    "\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        # Creates an empty array for the left dataset. This will hold all rows\n",
    "        # where the value in the specified feature column is less than or equal\n",
    "        # to the threshold.\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold]) \n",
    "        # Similarly, creates an empty array for the right dataset. This will\n",
    "        # contain all rows where the value in the specified feature column is\n",
    "        # greater than the threshold.\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        # Returns the two subsets. These will be used as the new datasets for\n",
    "        # the subsequent recursive calls to build the left and right subtrees,\n",
    "        # respectively.\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):  \n",
    "        # Calculate the proportion of samples that go to the left and right child nodes.\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        # If the mode is set to \"gini\", use the Gini impurity measure to calculate information gain.\n",
    "        if mode == \"gini\":\n",
    "            # Information gain is the impurity of the parent node minus the weighted sum of the impurity\n",
    "            # of the left and right child nodes.\n",
    "            gain = self.gini_index(parent) - (weight_l * self.gini_index(l_child) + weight_r * self.gini_index(r_child))\n",
    "        else:\n",
    "            # If the mode is not \"gini\" (default is \"entropy\"), use the entropy measure to calculate information gain.\n",
    "            gain = self.entropy(parent) - (weight_l * self.entropy(l_child) + weight_r * self.entropy(r_child))\n",
    "        \n",
    "        return gain\n",
    "\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        # Identify the unique class labels in the dataset.\n",
    "        class_labels = np.unique(y)\n",
    "        # Initialize entropy to zero. Entropy will accumulate the disorder measure for each class.\n",
    "        entropy = 0\n",
    "        # Iterate over each class label. \n",
    "        for cls in class_labels:\n",
    "            # Calculate the proportion (p_cls) of instances in the dataset belonging to the current class.\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            # Update the entropy using the entropy formula part associated with this class.\n",
    "            # The formula for entropy of a class is: -p * log2(p), where p is the class proportion.\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        # Return the total entropy, which represents the disorder within the entire dataset.\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    def gini_index(self, y):      \n",
    "        # Identify the unique class labels in the dataset.\n",
    "        class_labels = np.unique(y)\n",
    "        # Initialize gini to zero. This will accumulate the squared proportion of each class.\n",
    "        gini = 0\n",
    "        # Iterate over each class label.\n",
    "        for cls in class_labels:\n",
    "            # Calculate the proportion (p_cls) of instances in the dataset belonging to the current class.\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            # Accumulate the square of the proportion of the current class to the gini score.\n",
    "            gini += p_cls**2\n",
    "        # Gini impurity is calculated as 1 minus the accumulated score.\n",
    "        # This transformation ensures that a higher value indicates higher impurity.\n",
    "        return 1 - gini\n",
    "\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        if len(Y) == 0:  # Check if Y is empty\n",
    "            # Return the most common class of the entire dataset\n",
    "            return self.most_common_class\n",
    "        # Convert Y to a flat list of labels if it's not already\n",
    "        flat_Y = Y.flatten()\n",
    "        # Calculate the most common class label in Y\n",
    "        return np.bincount(flat_Y).argmax()\n",
    "\n",
    "\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''     \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def _make_prediction(self, x, tree):\n",
    "        if tree.value is not None:\n",
    "            # Ensure tree.value is an index into self.label_mapping.\n",
    "            # If tree.value is already an original class label, this line will raise KeyError if the label doesn't exist in the mapping.\n",
    "            return self.label_mapping.get(tree.value, tree.value)  # Provide a fallback if the key doesn't exist\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self._make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self._make_prediction(x, tree.right)\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''     \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "    def prune(self, X_val, Y_val, epsilon=0.5):\n",
    "        \"\"\"\n",
    "        Prune the decision tree using post-pruning method.\n",
    "        \"\"\"\n",
    "        self._prune_node(self.root, X_val, Y_val, epsilon)\n",
    "\n",
    "\n",
    "\n",
    "    def _prune_node(self, node, X_val, Y_val, epsilon):\n",
    "        \"\"\"\n",
    "        Recursively prune the tree.\n",
    "        \"\"\"\n",
    "        if node.left is not None or node.right is not None:\n",
    "            # Prune child nodes first\n",
    "            if node.left is not None:\n",
    "                self._prune_node(node.left, X_val, Y_val, epsilon)\n",
    "            if node.right is not None:\n",
    "                self._prune_node(node.right, X_val, Y_val, epsilon)\n",
    "\n",
    "            # After pruning children, check if current node can be pruned\n",
    "            if node.left is not None and node.right is not None:\n",
    "                if node.left.value is not None and node.right.value is not None:\n",
    "                    # Calculate error before pruning\n",
    "                    error_before = self._evaluate_error(X_val, Y_val)\n",
    "                    \n",
    "                    # Temporarily make current node a leaf\n",
    "                    original_left, original_right = node.left, node.right\n",
    "                    node.left, node.right = None, None\n",
    "                    node.value = self.calculate_leaf_value(Y_val[node.feature_index <= node.threshold])\n",
    "\n",
    "                    # Calculate error after pruning\n",
    "                    error_after = self._evaluate_error(X_val, Y_val)\n",
    "\n",
    "                    # Revert pruning if it does not reduce error\n",
    "                    if error_before - error_after < epsilon:\n",
    "                        node.left, node.right = original_left, original_right\n",
    "                        node.value = None\n",
    "    \n",
    "    def _map_labels(self, Y):\n",
    "        # Map labels to a continuous range of non-negative integers\n",
    "        unique_labels, Y_mapped = np.unique(Y, return_inverse=True)\n",
    "        self.label_mapping = {i: label for i, label in enumerate(unique_labels)}\n",
    "        return self.label_mapping, Y_mapped.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3567722",
   "metadata": {},
   "source": [
    "## Purning method : \n",
    "For each node, the method determines if changing it into a leaf (by deleting its children) improves or preserves tree accuracy on a separate validation set (X_val, Y_val).This is found by comparing the error before and after trimming the node, as estimated in the _evaluate_error function. If the change in error is smaller than a modest threshold epsilon, suggesting that pruning does not have a large impact on the model's performance, the pruning is retained; the node's children are removed, transforming it into a leaf node with a class label defined by the majority class of the data points in it. In general it should reducing overfitting and improving its performance when dealing unseen data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f02e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'website-phishing.csv'\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=41)\n",
    "\n",
    "\n",
    "# Initialize and fit the classifier\n",
    "classifier = PurningDecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prune the tree\n",
    "classifier.prune(X_val, y_val)\n",
    "# Evaluate accuracy\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy after pruning: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf19287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43542e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'BCP.csv'\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "X = data.iloc[:, 1:-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bff87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=41)\n",
    "\n",
    "\n",
    "# Initialize and fit the classifier\n",
    "classifier = PurningDecisionTreeClassifier(min_samples_split=2, max_depth=8)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prune the tree\n",
    "classifier.prune(X_val, y_val)\n",
    "# Evaluate accuracy\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy after pruning: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'new_arrhy.csv'\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "X = data.iloc[:, 1:-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "X = X.astype(np.int64)\n",
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=41)\n",
    "\n",
    "\n",
    "# Initialize and fit the classifier\n",
    "classifier = PurningDecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prune the tree\n",
    "classifier.prune(X_val, y_val)\n",
    "# Evaluate accuracy\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy after pruning: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f139dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c3a60",
   "metadata": {},
   "source": [
    "After running tree code above that we can tell taht the purning slidy improved performance on web-phishing and BCP, but on the arrhythmia dataset, the accuracy rate drop by some reason, that could be following reasons: \n",
    "- Overfitting \n",
    "- Purning method cut relevant substree\n",
    "- Dataset is too complexity \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading website-phishing \n",
    "file_path = 'website-phishing.csv'\n",
    "web = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "file_path = 'BCP.csv'\n",
    "bcp = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "file_path = 'new_arrhy.csv'\n",
    "arrhy = pd.read_csv(file_path, skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = web.iloc[:, 1:-1].values\n",
    "y = web.iloc[:, -1].values.reshape(-1, 1)\n",
    "X_train, X_test,y_train,y_test1 = train_test_split(X,y, test_size=0.2, random_state=41)\n",
    "classifier1 = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier1.fit(X_train, y_train.ravel())  # Assuming y_train is a 2D array for sklearn compatibility\n",
    "y_pred1 = classifier1.predict(X_test)\n",
    "accuracy1 = accuracy_score(y_test1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test2 = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=41)\n",
    "\n",
    "classifier2 = PurningDecisionTreeClassifier(min_samples_split = 2, max_depth=5)\n",
    "classifier2.fit(X_train, y_train)\n",
    "classifier2.prune(X_val, y_val)\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "\n",
    "accuracy2 = accuracy_score(y_test2, y_pred2)\n",
    "print(f\"Accuracy of Decision Tree: {accuracy1}\")\n",
    "print(f\"Accuracy of Decision Tree with purning: {accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy (y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "y_pred1 = np.array(y_pred1)\n",
    "y_pred2 = np.array(y_pred2)\n",
    "diff_accuracies = []\n",
    "n_iterations = 1000\n",
    "n_size = int(len(y_test1) * 0.50)  # Use 50% of the dataset for each bootstrap sample\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Prepare train and test sets\n",
    "    indices = resample(np.arange(len(y_test1)), n_samples=n_size)\n",
    "    y_test_sample = y_test1[indices]\n",
    "    y_pred1_sample = y_pred1[indices]\n",
    "    y_pred2_sample = y_pred2[indices]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy1 = calcuate_accuracy(y_test_sample, y_pred1_sample)\n",
    "    accuracy2 = calcuate_accuracy(y_test_sample, y_pred2_sample)\n",
    "    diff_accuracies.append(accuracy1 - accuracy2)\n",
    "\n",
    "# Calculate p-value\n",
    "diff_accuracies = np.array(diff_accuracies)\n",
    "# Count how many times the difference in accuracies was at least as extreme as the observed difference\n",
    "observed_diff = accuracy1 - accuracy2\n",
    "p_value = np.sum(np.abs(diff_accuracies) >= np.abs(observed_diff)) / n_iterations\n",
    "\n",
    "print(f'Estimated p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bcp.iloc[:, 1:-1].values\n",
    "y = bcp.iloc[:, -1].values.reshape(-1, 1)\n",
    "X_train, X_test,y_train,y_test1 = train_test_split(X,y, test_size=0.2, random_state=41)\n",
    "classifier1 = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier1.fit(X_train, y_train.ravel())  # Assuming y_train is a 2D array for sklearn compatibility\n",
    "y_pred1 = classifier1.predict(X_test)\n",
    "accuracy1 = accuracy_score(y_test1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test2 = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=41)\n",
    "\n",
    "classifier2 = PurningDecisionTreeClassifier(min_samples_split = 2, max_depth=5)\n",
    "classifier2.fit(X_train, y_train)\n",
    "classifier2.prune(X_val, y_val)\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "\n",
    "accuracy2 = accuracy_score(y_test2, y_pred2)\n",
    "print(f\"Accuracy of Decision Tree: {accuracy1}\")\n",
    "print(f\"Accuracy of Decision Tree with purning: {accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy (y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "y_pred1 = np.array(y_pred1)\n",
    "y_pred2 = np.array(y_pred2)\n",
    "diff_accuracies = []\n",
    "n_iterations = 1000\n",
    "n_size = int(len(y_test1) * 0.50)  # Use 50% of the dataset for each bootstrap sample\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Prepare train and test sets\n",
    "    indices = resample(np.arange(len(y_test1)), n_samples=n_size)\n",
    "    y_test_sample = y_test1[indices]\n",
    "    y_pred1_sample = y_pred1[indices]\n",
    "    y_pred2_sample = y_pred2[indices]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy1 = calcuate_accuracy(y_test_sample, y_pred1_sample)\n",
    "    accuracy2 = calcuate_accuracy(y_test_sample, y_pred2_sample)\n",
    "    diff_accuracies.append(accuracy1 - accuracy2)\n",
    "\n",
    "# Calculate p-value\n",
    "diff_accuracies = np.array(diff_accuracies)\n",
    "# Count how many times the difference in accuracies was at least as extreme as the observed difference\n",
    "observed_diff = accuracy1 - accuracy2\n",
    "p_value = np.sum(np.abs(diff_accuracies) >= np.abs(observed_diff)) / n_iterations\n",
    "\n",
    "print(f'Estimated p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = arrhy.iloc[:, 1:-1].values\n",
    "y = arrhy.iloc[:, -1].values.reshape(-1, 1)\n",
    "X_train, X_test,y_train,y_test1 = train_test_split(X,y, test_size=0.2, random_state=41)\n",
    "classifier1 = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
    "classifier1.fit(X_train, y_train.ravel())  \n",
    "y_pred1 = classifier1.predict(X_test)\n",
    "accuracy1 = accuracy_score(y_test1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0de9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'new_arrhy.csv'\n",
    "data = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "X = data.iloc[:, 1:-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "X = X.astype(np.int64)\n",
    "y = y.astype(np.int64)\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test2 = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=41)\n",
    "\n",
    "classifier2 = PurningDecisionTreeClassifier(min_samples_split = 2, max_depth=5)\n",
    "classifier2.fit(X_train, y_train)\n",
    "classifier2.prune(X_val, y_val)\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "\n",
    "accuracy2 = accuracy_score(y_test2, y_pred2)\n",
    "print(f\"Accuracy of Decision Tree: {accuracy1}\")\n",
    "print(f\"Accuracy of Decision Tree with purning: {accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy (y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "y_pred1 = np.array(y_pred1)\n",
    "y_pred2 = np.array(y_pred2)\n",
    "diff_accuracies = []\n",
    "n_iterations = 1000\n",
    "n_size = int(len(y_test1) * 0.50)  # Use 50% of the dataset for each bootstrap sample\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Prepare train and test sets\n",
    "    indices = resample(np.arange(len(y_test1)), n_samples=n_size)\n",
    "    y_test_sample = y_test1[indices]\n",
    "    y_pred1_sample = y_pred1[indices]\n",
    "    y_pred2_sample = y_pred2[indices]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy1 = calcuate_accuracy(y_test_sample, y_pred1_sample)\n",
    "    accuracy2 = calcuate_accuracy(y_test_sample, y_pred2_sample)\n",
    "    diff_accuracies.append(accuracy1 - accuracy2)\n",
    "\n",
    "# Calculate p-value\n",
    "diff_accuracies = np.array(diff_accuracies)\n",
    "# Count how many times the difference in accuracies was at least as extreme as the observed difference\n",
    "observed_diff = accuracy1 - accuracy2\n",
    "p_value = np.sum(np.abs(diff_accuracies) >= np.abs(observed_diff)) / n_iterations\n",
    "\n",
    "print(f'Estimated p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548c218",
   "metadata": {},
   "source": [
    "After computing the p-values for comparisons of decision trees with and without pruning across three datasets, we discovered that they are consistently high. This shows that, in this scenario, the pruning strategy does not greatly improve decision tree performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
